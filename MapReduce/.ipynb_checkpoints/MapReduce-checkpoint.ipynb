{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Map Reduce\n",
    "\n",
    "Eerst maken we een aparte directory aan voor alles wat we voor deze notebook gaan gebruiken in hdfs. Dit om conflicten of het overschrijven van gegevens te vermijden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pydoop.hdfs as hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "localFS = hdfs.hdfs(host='')\n",
    "client = hdfs.hdfs(host='localhost', port=9000)\n",
    "\n",
    "if not client.exists('/user/bigdata/05_MapReduce'):\n",
    "    client.create_directory('/user/bigdata/05_MapReduce')\n",
    "client.set_working_directory('/user/bigdata/05_MapReduce')\n",
    "print(client.working_directory())\n",
    "\n",
    "# do some cleaning in case anything else than input.txt is present\n",
    "for f in client.list_directory(\".\"):\n",
    "    if not f[\"name\"].endswith(\"input.txt\"):\n",
    "        client.delete(f[\"name\"], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Wat is MapReduce\n",
    "\n",
    "MapReduce is een programmeermodel om eenvoudig distributed data te verwerken.\n",
    "Het is belangrijk om te realiseren dat de programma's die je hier schrijft een parallel uitgevoerd worden op verschillende stukjes data (De map-fase) om daarna in de reduce-fase tot een finale output teruggebracht te worden.\n",
    "Er gebeuren 5 stappen bij het uitvoeren van een MapReduce programma\n",
    "* Bepalen op welke nodes de code uitgevoerd wordt (wordt door YARN gedaan afhankelijk van de locatie van de blocks)\n",
    "* Uitvoeren van de Map-code (Geschreven door de developer naar eigen wens)\n",
    "* Shuffle, ouput van de map-fase doorsturen naar andere nodes die de resultaten gaan reduceren (Automatisch)\n",
    "* uitvoeren van de Reduce-code (Geschreven door de developer naar eigen wens)\n",
    "* Combineren van de reduce output tot 1 gehele/finale output (Automatisch)\n",
    "\n",
    "Uit bovenstaand stappenplan is het duidelijk dat er twee zaken moeten geimplementeerd worden bij het schrijven van een MapReduce toepassing.\n",
    "Echter zullen we eerst een aantal voorbeelden bestuderen met reeds bestaande implementaties om zo meer vertrouwd te geraken met de flow van MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Voorbeelden van bestaande applicaties\n",
    "\n",
    "Reeds een aantal default MapReduce applications zijn mee geinstalleerd met Hadoop.\n",
    "De jar die deze toepassingen bevat kan gevonden worden in hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar.\n",
    "Wanneer je deze jar uitvoert met onderstaande commando krijg je een lijst met de beschikbare applicaties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!hadoop jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In deze notebook gaan we vooral focussen op het typische probleem van wordcount.\n",
    "Dit is een toepassing dat gaat tellen hoe vaak elk woord voorkomt in een bepaalde tekst.\n",
    "Om meer informatie over deze toepassing te krijgen kan je gebruik maken van het volgende commando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!hadoop jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Voor we deze applicatie kunnen uitvoeren moeten we eerst ervoor zorgen dat er input data beschikbaar is in HDFS.\n",
    "Upload hiervoor de input.txt file met onderstaande code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "localFS.copy(\"input.txt\", client, \"input.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Het toevoegen van dit bestand kan geverifieerd worden door het HDFS te gaan bekijken op volgende link [http://localhost:9870/explorer.html#/user/bigdata/05_MapReduce](http://localhost:9870/explorer.html#/user/bigdata/05_MapReduce)\n",
    "\n",
    "Met onderstaande commando kan nu het precompiled word count example uitgevoerd worden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!hadoop jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar wordcount 05_MapReduce/input.txt 05_MapReduce/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Indien het bovenstaande commando een foutmelding geeft over het niet vinden van org.apache.hadoop.mapreduce.v2.app.MRAppMaster, voeg het volgende toe aan hadoop/etc/hadoop/mapred-site.xml\n",
    "\n",
    "    <property>\n",
    "\t  \t<name>yarn.app.mapreduce.am.env</name>\n",
    "\t  \t<value>HADOOP_MAPRED_HOME=/home/bigdata/hadooop</value>\n",
    "\t</property>\n",
    "\t<property>\n",
    "\t  <name>mapreduce.map.env</name>\n",
    "\t  <value>HADOOP_MAPRED_HOME=/home/bigdata/hadooop</value>\n",
    "\t</property>\n",
    "\t<property>\n",
    "\t  <name>mapreduce.reduce.env</name>\n",
    "\t  <value>HADOOP_MAPRED_HOME=/home/bigdata/hadooop</value>\n",
    "\t</property>\n",
    "\n",
    "Na dit toe te voegen, herstart yarn en hdfs.\n",
    "Indien het hier nog niet mee opgelost is voer de volgende stappen uit:\n",
    "* Voer \"hadoop classpath\" uit in de terminal\n",
    "* Voeg het volgende toe aan hadoop/etc/hadoop/yarn-site.xml:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "    <property>\n",
    "        <name>yarn.application.classpath</name>\n",
    "        <value>output van stap 1</value>\n",
    "    </property>\n",
    "\n",
    "Na het uitvoeren van het wordcount applicatie moet er een output.txt file aangemaakt zijn met de resultaten. **Bekijk dit nu.**\n",
    "Het resultaat is een file met key-value pairs met de woorden als key en de frequentie/aantal keer dat de woorden voorkomen als key.\n",
    "\n",
    "Probeer nu op basis van bovenstaande reeds bestaande applicaties de gemiddelde lengte van de woorden in de text te bepalen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# uit te voeren op commando voor de gemiddelde lengte van de woorden te bekomen\n",
    "!hadoop jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar wordmean 05_MapReduce/input.txt 05_MapReduce/output_mean_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "De resource manager houdt ook een overzicht bij van de uitgevoerde applicaties, hun status, runtime en eventuele loggings. Na bovenstaande commando's uit te voeren moet je iets gelijkaardigs zien als in de onderstaande screenshot.\n",
    "\n",
    "![yarn of mapreduce](images/yarn_001.png)\n",
    "\n",
    "## Zelf implementeren van MapReduce applicaties\n",
    "\n",
    "Natuurlijk zijn er veel meer zaken mogelijk om te berekenen met map-reduce toepassingen dan de reeds gecompileerde in hadoop.\n",
    "Zoals eerder aangehaald valt vooral het coderen van de Map- en Reducestap hierbij op de schouders van de developer.\n",
    "De standaard programmeertaal van MapReduce is Java en dus ook het grootste deel van de documentatie over MapReduce is geschreven met behulp van Java.\n",
    "Deze programmas moeten dan gecompileerd worden tot een jar dat geupload kan worden naar de overeenkomstige nodes en daar uitgevoerd.\n",
    "De api overview van hadoop kan je [hier](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/package-summary.html) vinden.\n",
    "\n",
    "De code voor het wordcount example ziet er als volgt uit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%file WordCount.java\n",
    "// dit schrijft onderstaande cell naar een file in de huidige directory\n",
    "import java.io.IOException;\n",
    "import java.util.StringTokenizer;\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "public class WordCount {\n",
    "\n",
    "    // Mapper de classes na extends Mapper<> zijn input-key, input-value, output-key, output-value\n",
    "    public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable>{\n",
    "\n",
    "        private final static IntWritable one = new IntWritable(1);\n",
    "        private Text word = new Text();\n",
    "\n",
    "        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n",
    "            \n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Reducer: de classes na extends Reducer<> zijn input-key, input-value, output-key, output-value\n",
    "    public static class IntSumReducer extends Reducer<Text,IntWritable,Text,IntWritable> {\n",
    "        private IntWritable result = new IntWritable();\n",
    "\n",
    "        public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n",
    "         \n",
    "        }\n",
    "    }\n",
    "\n",
    "    // configure the MapReduce program\n",
    "    public static void main(String[] args) throws Exception {\n",
    "      \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Deze code bevat drie delen:\n",
    "* De main() functie: verzorgt de configuratie van de uit te voeren taak. Geeft aan wat de Map en Reduce klassen zijn, wat de input is, hoe de output bewaard wordt ,...\n",
    "* De Map-klasse met de map() functie bevat de code voor de mapping-fase\n",
    "* De Reduce-klasse met de reduce() functie bevat de code voor de reduce-fase\n",
    "\n",
    "De laatste twee klassen zijn hier gecodeerd als geneste klassen. Deze hadden ook in aparte files geplaatst kunnen worden.\n",
    "Nu moet deze code eerst omgezet/gecompileerd worden tot een .jar file. Deze kan dan analoog uitgevoerd worden als hierboven met het voorbeeldcode.\n",
    "Deze twee stappen kunnen uitgevoerd worden door onderstaande commando's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# compileren tot .jar\n",
    "!javac -d . WordCount.java\n",
    "!jar cvf wordcounter.jar *.class\n",
    "# execute\n",
    "!hadoop jar wordcounter.jar WordCount 05_MapReduce/input.txt 05_MapReduce/output_java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Oefening\n",
    "\n",
    "Kopieer nu bovenstaand java programma en pas het aan zodat het het aantal worden telt dat begint met elke letter van het alfabet (zet eerst de woorden om naar kleine letters).\n",
    "Hiervoor moet je dus het resultaat van de itr.nextToken() eerst gaan omvormen om er enkel de eerste letter eruit te halen en deze om te zetten naar lowercase).\n",
    "De nodige functies om dit resultaat kun je [hier](https://docs.oracle.com/javase/7/docs/api/java/lang/String.html) en [hier](https://docs.oracle.com/javase/8/docs/api/java/lang/Character.html) vinden.\n",
    "Indien dit gelukt is zorg er ook voor dat je geen entries toevoegt indien het eerste character geen letter is.\n",
    "Compileer dit programma naar een .jar en voer het uit.\n",
    "Controleer het resultaat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%file StartLetter.java\n",
    "import java.io.IOException;\n",
    "import java.util.StringTokenizer;\n",
    "import java.lang.*;\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "public class StartLetter {\n",
    "\n",
    "    public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable>{\n",
    "\n",
    "        private final static IntWritable one = new IntWritable(1);\n",
    "        private Text word = new Text();\n",
    "\n",
    "        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n",
    "            \n",
    "        }\n",
    "    }\n",
    "\n",
    "    public static class IntSumReducer extends Reducer<Text,IntWritable,Text,IntWritable> {\n",
    "        private IntWritable result = new IntWritable();\n",
    "\n",
    "        public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n",
    "           \n",
    "        }\n",
    "    }\n",
    "\n",
    "    public static void main(String[] args) throws Exception {\n",
    "       \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# compileren tot .jar\n",
    "!javac -d . StartLetter.java\n",
    "!jar cvf startletter.jar *.class # let op, dit gaat wordcount classes meenemen in de jar dus met het eerste argument kan je kiezen welke main gestart wordt\n",
    "# execute\n",
    "!hadoop jar startletter.jar StartLetter 05_MapReduce/input.txt 05_MapReduce/output_java_exer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Implementeren via Python\n",
    "\n",
    "Hoewel het Hadoop Ecosysteem geprogrammeerd is in Java en het dus er goed mee samenwerkt, is het niet verplicht om Java te gebruiken om MapReduce applicaties te schrijven.\n",
    "Java krijgt namelijk veel kritiek, vooral doordat er veel code nodig is om eenvoudige zaken te programmeren.\n",
    "Om andere programmeertalen te gebruiken worden er verscheidene API's aangeboden door Hadoop, namelijk\n",
    "* Hadoop Streaming\n",
    "    * Communicatie via stdin/stdout\n",
    "    * Gebruikt door hadoopy, mrjob, ...\n",
    "* Hadoop Pipes\n",
    "    * C++ interface voor Hadoop\n",
    "    * Communicatie via sockets\n",
    "    * Gebruikt door pydoop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Mrjob\n",
    "\n",
    "De eerste API die we bekijken is MrJobs.\n",
    "Deze API maakt gebruik van de Hadoop Streaming API.\n",
    "De voordelen van MrJobs zijn:\n",
    "* Uitgebreidde documentatie\n",
    "* Code kan lokaal uitgevoerd worden als test\n",
    "* Data serialisatie gebeurt automatisch (nadeel van Streaming API)\n",
    "* Werkt met Amazon Elastic MapReduce en Google Cloud Dataproc\n",
    "\n",
    "Het grootste nadeel is dat de StreamingAPI niet de volledige kracht heeft van het Hadoop ecosysteem omdat alles omgezet wordt naar strings (jsons)\n",
    "\n",
    "Installatie van deze package gebeurt als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!echo bigdata | sudo -S pip install mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%file wordcount_mrjob.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deze code kan uitgevoerd worden door het commando hieronder.\n",
    "Let op dat dit een lokale file gebruikt voor de output.\n",
    "Indien je de output wil bewaren in het hdfs moet je dit nog uploaden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!python wordcount_mrjob.py -r hadoop hdfs:///user/bigdata/05_MapReduce/input.txt > output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Pydoop\n",
    "\n",
    "Het word-count example door gebruik te maken van pydoop ziet er uit als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%file wordcount_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dit script kan uitgevoerd worden op de cluster als volgt:\n",
    "\n",
    "**Let op:** Indien dit script heel lang blijft hangen op 0% is er waarschijnlijk iets misgegaan.\n",
    "De timeout die gebruikt wordt om de applicatie af te sluiten is 10 minuten.\n",
    "De snelste manier op de applicatie af te sluiten is door gebruik te maken van het commando:\n",
    "    yarn application -kill <application-id>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pydoop script wordcount_script.py 05_MapReduce/input.txt 05_MapReduce/output_python_script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bovenstaande code werkt door gebruik te maken van een script, dit maakt het mogelijk om eenvoudige applicaties te schrijven.\n",
    "Voor complexere zaken (vooral in het geval dat er een bepaalde state moet bijgehouden worden kunnen er ook Mapper en Reducer classes aangemaakt worden waarmee het mogelijk is om de volledige [Pydoop API](https://crs4.github.io/pydoop/tutorial/mapred_api.html#api-tutorial) te gebruiken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%file wordcount_pydoop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pydoop submit --upload-file-to-cache wordcount_pydoop.py wordcount_pydoop /user/bigdata/05_MapReduce/input.txt 05_MapReduce/output_python --entry-point main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let in het commando hierboven ook op de entry point parameter. Deze is belangrijk omdat het anders niet gaat werken**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oefening\n",
    "\n",
    "Pas nu ook bovenstaande python applicatie aan om het aantal woorden dat begint met elke letter te tellen. Let er ook hierbij op dat er geen verschil is tussen hoofd- en kleine letters.\n",
    "\n",
    "Indien dit goed gelukt is, probeer ook de applicatie aan te passen om per letter de gemiddelde lengte van de woorden te berekenen.\n",
    "Let er hierbij op dat het niet rechtstreeks mogelijk is om meerdere keren de volledige iterator met de values te gebruiken. \n",
    "Terwijl dit nodig is voor het gemiddelde te berekenen met behulp van de standaard methoden (len, sum, ...).\n",
    "Een oplossing hiervoor is om gebruik te maken van de itertools package\n",
    "\n",
    "    from itertools import tee\n",
    "    \n",
    "Meer informatie over het gebruik hiervan vind je [hier](https://www.geeksforgeeks.org/python-itertools-tee/).\n",
    "\n",
    "Daarnaast is het ook belangrijk om te beseffen dat je meerdere keys kan emitten per woord indien gewenst.\n",
    "Breid nu de applicatie uit om ook de maximum lengte van alle woorden te bepalen.\n",
    "Denk hierbij erover na hoe je het onderscheid kan maken tussen de twee soorten keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
