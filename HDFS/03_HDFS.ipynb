{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS\n",
    "\n",
    "HDFS is het distributed file system van Hadoop dat de basis vormt voor een breed gamma van applicaties, waaronder MapReduce.\n",
    "Dit framework maakt het mogelijk om niet-gespecialiseerde hardware te gebruiken om eenvoudig datacenters op te zetten of rekenclusters te beheren.\n",
    "HDFS bereikt dit doel op een aantal manieren:\n",
    "* Ten eerste wordt een file opgedeeld in verschillende blokken. Deze worden verdeeld over verschillende computers zodat code meerdere delen van het bestand kan gebruiken in parallel om zo de benodigde rekenkracht te verdelen over meerdere toestellen.\n",
    "* Daarnaast worden de blokken ook gedupliceerd om zo fout-toleranter te zijn in het geval van een crash (hardware of software), stroomstoring, netwerkonderbreking.\n",
    "\n",
    "Om dit te bereiken gebruikt Hadoop een Master-Slave architectuur dat bestaat uit een enkele namenode (master) en meerdere datanodes (slaves).\n",
    "De namenode houdt bij hoeveel datanodes er actief zijn, welke blokken ze hebben en welke blokken bij welke file horen.\n",
    "Indien er een datanode crasht gaat deze server dit detecteren en dit oplossen door de nodige blokken te kopieren van een andere datanode zodat er steeds voldoende kopies in het systeem aanwezig zijn.\n",
    "Bij elke actie die uitgevoerd moet worden in het HDFS moet er steeds gevraagd worden aan de namenode welke blokken op welke datanodes we nodig hebben voor de gewenste file uit te lezen of code voor uit te voeren.\n",
    "Het is dus duidelijk dat deze namenode een single-point-of-failure is wat ideaal is voor de availability van de cluster.\n",
    "Dit kan opgelost worden door HDFS te runnen in een high-availability mode wat ervoor zorgt dat er steeds een backup aanwezig is voor de namenode die zeer snel de werking kan overnemen.\n",
    "Deze structuur maakt het eenvoudig om aan horizontal scaling te doen door extra servers toe te voegen zonder dat er downtime is voor de hele cluster.\n",
    "\n",
    "Dit resulteer in de volgende kenmerken van HDFS:\n",
    "* High Throughput\n",
    "* Scalability\n",
    "* High Availability\n",
    "* Data Reliability\n",
    "* Fault Tolerance\n",
    "\n",
    "## Starten en stoppen van de cluster\n",
    "\n",
    "De cluster kan gestart worden door de volgende commando's uit te voeren in de commandline:\n",
    "* start-dfs.sh\n",
    "* start-yarn.sh\n",
    "\n",
    "Eenmaal dat de cluster gestart is kunnen de lopende java applicaties gecontroleerd worden door middel van het commando jps.\n",
    "Dit kan ook uitgevoerd worden vanuit deze jupyter notebook door onderstaande code-cell uit te voeren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [localhost]\n",
      "Starting datanodes\n",
      "Starting secondary namenodes [bigdata-VirtualBox]\n",
      "WARNING: YARN_CONF_DIR has been replaced by HADOOP_CONF_DIR. Using value of YARN_CONF_DIR.\n",
      "Starting resourcemanager\n",
      "WARNING: YARN_CONF_DIR has been replaced by HADOOP_CONF_DIR. Using value of YARN_CONF_DIR.\n",
      "Starting nodemanagers\n",
      "WARNING: YARN_CONF_DIR has been replaced by HADOOP_CONF_DIR. Using value of YARN_CONF_DIR.\n",
      "22529 DataNode\n",
      "22403 NameNode\n",
      "22707 SecondaryNameNode\n",
      "23237 Jps\n",
      "22953 ResourceManager\n",
      "23087 NodeManager\n"
     ]
    }
   ],
   "source": [
    "# uitroepingsteken zegt tegen jupyter notebook dat dit op de terminal uitgevoerd wordt\n",
    "\n",
    "!start-dfs.sh\n",
    "!start-yarn.sh\n",
    "#!start-all.sh\n",
    "!jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ook heeft de namenode een website waar naartoe kan gegaan worden om meer informatie over de cluster te bekijken. De standaardurl is [http://localhost:9870](http://localhost:9870).\n",
    "\n",
    "Om de cluster terug te stoppen moeten in de terminal of de notebook de volgende commando's uitgevoerd worden.\n",
    "* stop-dfs.sh\n",
    "* stop-yarn.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!stop-yarn.sh\n",
    "#!stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Communiceren met het HDFS\n",
    "\n",
    "Er zijn verschillende manieren om dit distributed bestandssysteem te gebruiken.\n",
    "Ten eerste kan je gebruik maken van een command line interface (CLI) om bestanden uit te lezen, op te laden, ...\n",
    "Daarnaast zijn er ook wrappers voor verschillende talen die toelaten om rechtstreeks vanuit code te interageren met het HDFS.\n",
    "De voordelen van deze wrappers over een CLI zijn:\n",
    "* Flexibiler om te interageren in applicaties.\n",
    "* Gebruiksvriendelijker en gemakkelijker om te automatiseren.\n",
    "* Eenvoudiger in onderhoud en te debuggen\n",
    "* Volledige functionaliteit van een programmeertaal kan gebruikt worden zoals OO.\n",
    "\n",
    "### Instantiering van een client\n",
    "\n",
    "(Niet nodig als je de commandline gebruikt)\n",
    "\n",
    "Veel verschillende talen beschikken over een wrapper om te communiceren met een HDFS.\n",
    "In deze cursus gaan we gebruik maken van [de pydoop](https://crs4.github.io/pydoop/index.html) wrapper.\n",
    "De eerste stap in het gebruik van de wrapper is te bepalen hoe de namenode van het hdfs gevonden kan worden.\n",
    "Dit gebeurt als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydoop.hdfs as hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 09:55:38,051 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31040933888"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect met het locale filesysteem\n",
    "localFs = hdfs.hdfs(host='')\n",
    "# connect met het hdfs (default port is 9000)\n",
    "client = hdfs.hdfs(host=\"localhost\", port=9000)\n",
    "\n",
    "client.capacity()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Aanmaken van files en directories\n",
    "\n",
    "Om nu bestanden en folders aan te maken op dit distributed file systeem kunnen onderstaande functies gebruikt worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# er wordt steeds vertrokken van de folder /user/bigdata\n",
    "\n",
    "#client.exists(\"09_Streaming\")\n",
    "\n",
    "# aanmaken via commandline\n",
    "# !hdfs dfs -mkdir -p 03_HDFS_term\n",
    "\n",
    "path = \"03_HDFS\"\n",
    "if not client.exists(path):\n",
    "    client.create_directory(path)\n",
    "    \n",
    "# zo moet je niet altijd het pad naar de directory meegeven\n",
    "# proefondervindelijk kan het zijn dat niet alle commando's hiernaar luisten\n",
    "client.set_working_directory(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# uploaden van files\n",
    "#hadoop fs -put /path/in/linux /path/in/hdfs/with/filename\n",
    "#hadoop fs -put davinci_notebooks.txt 03_HDFS_term/notebooks.txt\n",
    "#localFs.copy(\"ulysses.txt\", client, \"03_HDFS/book.txt\")\n",
    "# bij move is het origneel weg\n",
    "localFs.move(\"book.txt\", client, \"03_HDFS/book2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloaden van files\n",
    "#!hadoop fs -get 03_HDFS_term/notebooks.txt\n",
    "client.copy(\"03_HDFS/book.txt\", localFs, \"book.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Bekijken van het filesysteem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xef\\xbb\\xbfThe Project Gutenberg eBook of Ulysses, by James Joyce\\n'\n",
      "b'\\n'\n",
      "b'This eBook is for the use of anyone anywhere in the United States and\\n'\n",
      "b'most other parts of the world at no cost and with almost no restrictions\\n'\n",
      "b'whatsoever. You may copy it, give it away or re-use it under the terms\\n'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#client.list_directory(\".\")\n",
    "# niveau hoger\n",
    "#client.list_directory(\"..\")\n",
    "# data used\n",
    "#client.used()\n",
    "client.set_working_directory(\"/user/bigdata/03_HDFS\")\n",
    "client.working_directory()\n",
    "client.list_directory(\".\")\n",
    "client.default_block_size()\n",
    "\n",
    "# de eerst 5 regels\n",
    "f = client.open_file(\"book.txt\")\n",
    "for i in range(5):\n",
    "    print(f.readline())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aanpassen van het filesysteem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# rename\n",
    "#client.rename(\"book.txt\", \"boek.txt\")\n",
    "!hadoop fs -mv 03_HDFS/boek.txt 03_HDFS/book.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 03_HDFS/book.txt\r\n"
     ]
    }
   ],
   "source": [
    "# delete\n",
    "#client.delete(\"book2.txt\")\n",
    "!hdfs dfs -rm 03_HDFS/book.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.delete(\"/user/bigdata/03_HDF_term\")\n",
    "client.delete(\"/user/bigdata/03_HDFS_term\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oefening\n",
    "\n",
    "Schrijf python code dat controleert of een directory bestaat in het hdfs.\n",
    "Indien nee wordt de directory aangemaakt.\n",
    "Indien ja, worden alle files in de directory verwijderd om van een lege directory te starten.\n",
    "Upload daarna een tekst-bestand naar keuze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.set_working_directory(\"/user/bigdata\")\n",
    "path = \"03_HDFS\"\n",
    "if not client.exists(path):\n",
    "    client.create_directory(path)\n",
    "else:\n",
    "    # dit gaat de volledige directory verwijderen\n",
    "    #client.delete(path + \"/\")\n",
    "    \n",
    "    # dit is geen geldig pad dus error\n",
    "    # client.delete(path + \"/*\")\n",
    "    \n",
    "    for f in client.list_directory(\".\"):\n",
    "        # do not delete input file\n",
    "        if(f[\"name\"] != \"dataset.csv\"):\n",
    "            client.delete(f[\"name\"])\n",
    "\n",
    "localFs.copy(\"ulysses.txt\", client, \"03_HDFS/book.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
