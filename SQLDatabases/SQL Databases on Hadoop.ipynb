{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f971e510",
   "metadata": {},
   "source": [
    "# SQL Databases on hadoop\n",
    "\n",
    "Relationele Database Management Systemen zijn traditioneel verticaal schaalbaar.\n",
    "Dit conflicteert met Big Data toepassingen waar we de voorkeur geven aan horizontale schaalbaarheid om onder andere de kosten te verlagen, de beschikbare rekenkracht of performantie en fout-tolerantie te verbeteren.\n",
    "Hierdoor kunnen de reeds gekende RDBMS niet gebruikt worden.\n",
    "\n",
    "De volgende populaire applicaties kunnen gebruikt worden om SQL te gebruiken op hadoop:\n",
    "* Apache Hive\n",
    "* Cloudera Impala\n",
    "* Presto\n",
    "* Shark\n",
    "\n",
    "Hierna gaan we een kort overzicht geven van al deze applicaties en daarna focussen we op een toepassingen van Apache Hive.\n",
    "\n",
    "## Apache Hive\n",
    "\n",
    "Deze applicatie is ontwikkeld door Facebook als **datawarehouse framework** voor interne toepassingen en is snel zeer populair geworden om queries uit te voeren op Hadoop.\n",
    "Het is belangrijk om in gedachten te houden dat hoewel Hive een **SQL-like querying omgeving** aanbiedt, dat er in de achtergrond een **MapReduce methodologie** gebruikt wordt om de database te bevragen.\n",
    "Dit wil zeggen dat de **queries gecompileerd moeten worden naar MapReduce toepassingen**.\n",
    "Hive ondersteund ook gebruikers-gedefineerde functies en laat toe om gecomprimeerde data te verwerken.\n",
    "Momenteel wordt Hive verder verbeterd en uitgebreid door HortonWorks (Cloudera) dat een nieuwe backend aan het uitwerken is (Tez Project) om de responstijd van Hive te verbeteren.\n",
    "\n",
    "Voordelen:\n",
    "* Op bijna alle Hadoop installaties standaard geinstalleerd\n",
    "* Goede tool om te proberen door minimale start-investering (gratis)\n",
    "\n",
    "Nadelen:\n",
    "* Niet meest snelle manier door overhead van MapReduce (batch processing)\n",
    "* Enkel 4 file-formats ondersteund:\n",
    " * Text, SequenceFile, ORC, RCFile\n",
    "\n",
    "## Cloudera Impala\n",
    "\n",
    "Maakt het mogelijk om interactieve SQL queries uit te voeren op HDFS en HBase. \n",
    "Impala voert **queries uit in real time** en verbeterd daardoor de performantie  door geen batch processing te gebruiken.\n",
    "Daarnaast wordt ook het **gebruik van verscheidene SQL-based bedrijfsanalyse tools mogelijk** gemaakt.\n",
    "Deze applicatie is een open source applicatie ontwikkeld door Cloudera.\n",
    "\n",
    "Voordelen:\n",
    "* Sneller dan Hive\n",
    "* Ondersteund cloud based architecture door Amazon's Elastic MapReduce\n",
    "* Is compatibel met ANSI SQL (standaard SQL standaard)\n",
    "* Integratie met business intelligence tools mogelijk\n",
    "\n",
    "Nadelen:\n",
    "* Moeilijker op te zetten\n",
    "* Volledige kracht maar beschikbaar bij gebruik van Parquet file format\n",
    "* Geen support voor YARN\n",
    "* Vereist installatie van daemons op elke node\n",
    "\n",
    "## Presto\n",
    "\n",
    "Een tweede applicatie ontwikkeld door Facebook.\n",
    "Ook deze applicatie is open source.\n",
    "Deze applicatie is geschreven in Java en heeft een groot aantal kenmerken gemeen met Impala, bijvoorbeeld:\n",
    "* Een interactieve ervaring\n",
    "* Moeiljk om op te zetten (installatie op de verscheidene nodes)\n",
    "* Vereist een specifiek file format voor data opslag (RCFile)\n",
    "\n",
    "Daarnaast biedt Presto wel compatibiliteit met de Hive meta-store en laat Presto toe om data van verscheidene bronnen te combineren.\n",
    "Het grootste verschil met Impala is dat Presto niet ondersteund wordt door veel leverancies van cloud-toepassingen, ook al maken reeds een aantal grote bedrijven er gebruik van (bijvoorbeeld AirBnb en Dropbox).\n",
    "\n",
    "## Shark\n",
    "\n",
    "Deze applicatie is ontstaan om een alternatief te bieden voor Hive met MapReduce.\n",
    "Het doel was om alle functionaliteiten van Hive te behouden maar de performantie te verbeteren.\n",
    "Deze tool is geschreven in Scala door UC Berkeley en zoals de naam doet vermoeden maakt het gebruik van **Spark**.\n",
    "Tot op een zeker punt kan Shark de performantie van Hive verbeteren maar de **schaalbaarheid van de tool** is niet zo goed als Hive.\n",
    "Dit komt omdat het **gebouwd is boven op Hive** waardoor het de complexe codebase van Hive heeft overgeerfd heeft.\n",
    "Het onderhoud en aanpassen van deze codebase zonder in te boeten op performantie is echter niet eenvoudig.\n",
    "\n",
    "## Spark SQL\n",
    "\n",
    "Dit onderdeel van Spark biedt de mogelijkheid aan om Spark Queries uit te voeren op ingeladen Dataframes.\n",
    "Omdat dit gebruik maakt van Spark biedt het veel voordelen en is de performantie beter dan tools die gebruik maken van MapReduce.\n",
    "Het grootste nadeel is echter dat deze data niet standaard opgeslagen wordt op een harde schrijf.\n",
    "Het is vrij eenvoudig om deze dataframes/tabellen op te slaan als bijvoorbeeld csv maar de relaties tussen kolommen van verschillende tabellen kan niet opgeslaan worden en vereist extra manueel werk om bij te houden.\n",
    "\n",
    "## Voorbeeld toepassing: Hive\n",
    "\n",
    "Om te beginnen moet Hive geinstalleerd worden.\n",
    "Hiervoor kan je de stappen volgen op [deze pagina](https://phoenixnap.com/kb/install-hive-on-ubuntu).\n",
    "Neem hiervoor de laatste versie (en de standaard hive, niet storage) en kies ook voor de reeds gecompileerde files(bin), niet de download met source files.\n",
    "De meest recente versie op het moment van het schrijven van deze file en de versie die ik geinstalleerd heb staan is  hive 3.1.2 met deze download file: apache-hive-3.1.2-bin.tar.gz.\n",
    "Let bij het installeren dat je de juiste paden invult naar de gedownloade folder (ik heb een hive folder aangemaakt in mijn home directory).\n",
    "\n",
    "**Let op: Het laatste deel van stap 5 (aanpassen van de hive.metastore.warehouse.dir property) moet niet uitgevoerd worden. Wacht met het uitvoeren van stap 6 tot onderstaande problemen zijn opgelost**\n",
    "\n",
    "**Probleem 1: gebruik van oudere versies door Hive**\n",
    "\n",
    "Ten eerste maakt Hive gebruik van een oudere versie van guave.\n",
    "Hoe je dit kan oplossen staat na de beschrijving van stap 6.\n",
    "\n",
    "Ook is Hive gecompileerd voor Java 8 en standaard is Java 11 geinstalleerd.\n",
    "Installeer nu Java 8 met het volgende commando\n",
    "\n",
    "    sudo apt install openjdk-8-jdk\n",
    "    \n",
    "Gebruik dan het onderstaande commando om de default versie op het hele systeem aan te passen (kies hierbij versie 8)\n",
    "\n",
    "    sudo update-alternatives --config java\n",
    "    \n",
    "Ten slotte open je in de home het .bashrc file en pas je de JAVA_HOME variabele aan naar versie 8.\n",
    "Vergeet niet deze aanpassingen door te voeren door middel van het volgende commando (indien je terminal in je home directory staat).\n",
    "\n",
    "    source .bashrc\n",
    "    \n",
    "Een laatste aanpassing voor java is dat de link in ~/hadoop/etc/hadoop/hadoop-env.sh nog niet aangepast is. Pas het pad naar de java versie 8 aan helemaal onderaan deze file.\n",
    "Doe dit ook voor de spark-env.sh. \n",
    "Deze file bestaat nog niet maar een template kan je vinden in hadoop/spark.../conf.\n",
    "Kopieer deze template en hernoem ze naar spark-env.sh.\n",
    "Restart nu het hdfs zodat de aanpassingen meegenomen worden.\n",
    "\n",
    "**Probleem 2: illegal character**\n",
    "\n",
    "Een ander probleem is dat de nieuwe guave versie die we gebruiken een bepaald character niet herkend in de hive-site.xml. \n",
    "Dit karakter (4 symbolen begint met en &) kan je vinden op rij 3215 van hive-site.xml.\n",
    "Aangezien dit karakter in een eenvoudige beschrijving staat kan het verwijderd worden zonder problemen.\n",
    "Verwijder dit karakter en sla je aanpassingen op. \n",
    "\n",
    "**Probleem 3: instellen user name**\n",
    "\n",
    "Hive moet ook nog weten als welke user het moet inloggen op het hdfs.\n",
    "Plaats hiervoor het volgende in ~/hive/conf/hive-site.xml\n",
    "    \n",
    "    <property>\n",
    "        <name>system:java.io.tmpdir</name>\n",
    "        <value>/tmp/hive/java</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>system:user.name</name>\n",
    "        <value>${user.name}</value>\n",
    "    </property>    \n",
    "    \n",
    "**Ga nu verder met de installatie door het uitvoeren van stap 6**\n",
    "\n",
    "### Hive via commandline\n",
    "\n",
    "We kunnen nu hive gebruiken via commandline door in de terminal het volgende commando uit te voeren\n",
    "\n",
    "    hive\n",
    "    \n",
    "Dit opent een sql-like console waarin we allerhande queries kunnen schrijven.\n",
    "Een uitgebreide beschrijving van alle mogelijke queries vind je [hier](https://cwiki.apache.org/confluence/display/hive/languagemanual).\n",
    "Bijvoorbeeld kunnen we de databases bekijken met\n",
    "\n",
    "    show databases;\n",
    "    \n",
    "Maak ook een database aan waarin we we gaan werken deze les.\n",
    "Schrijf hieronder de commando's die je nodig hebt om dit te doen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbdbd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!create database odisee;\n",
    "!use odisee;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b197ba29",
   "metadata": {},
   "source": [
    "Download nu deze zip files over [werknemers](https://github.com/RobinDong/hive-examples/blob/master/employee/employees.csv.gz) en [salarissen ](https://github.com/RobinDong/hive-examples/blob/master/employee/salaries.csv.gz).\n",
    "Unzip daarna deze files en upload ze naar het hdfs.\n",
    "Schrijf hieronder de nodige code om deze files in het hdfs op te slaan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "280694ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 15:29:09,657 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydoop.hdfs as hdfs\n",
    "\n",
    "localFS = hdfs.hdfs(host='')\n",
    "client = hdfs.hdfs(host='localhost', port=9000)\n",
    "\n",
    "if not client.exists('/user/bigdata/10_SQL'):\n",
    "    client.create_directory('/user/bigdata/10_SQL')\n",
    "\n",
    "# do some cleaning in case anything is present\n",
    "for f in client.list_directory(\".\"):\n",
    "    client.delete(f[\"name\"], True)\n",
    "        \n",
    "# upload input.txt\n",
    "localFS.copy(\"/home/bigdata/Downloads/employees.csv\", client, \"10_SQL/employees/employees.csv\")\n",
    "localFS.copy(\"/home/bigdata/Downloads/salaries.csv\", client, \"10_SQL/salaries/salaries.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b724b1",
   "metadata": {},
   "source": [
    "Deze files bevatten de data die we straks gaan inlezen in het Hive Datawarehouse. \n",
    "Hiervoor moeten we echter eerst de tabellen aanmaken waarin we deze data gaan opslaan.\n",
    "Dit kan door middel van de volgende HQL commando's uit te voeren in de hive HQL.\n",
    "\n",
    "Na het maken kunnen we de data inlezen door middel van het load data commando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d802f0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "create external table employee (\n",
    "    employee_id INT,\n",
    "    birthday DATE,\n",
    "    first_name STRING,\n",
    "    family_name STRING,\n",
    "    gender CHAR(1),\n",
    "    work_day DATE)\n",
    "row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "stored as textfile;\n",
    "\n",
    "LOAD DATA INPATH '/user/bigdata/10_SQL/employees' overwrite into table employee;\n",
    "\n",
    "create external table salary (\n",
    "    employee_id INT,\n",
    "    salary INT,\n",
    "    start_date DATE,\n",
    "    end_date DATE)\n",
    "row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "stored as textfile\n",
    "\n",
    "LOAD DATA INPATH '/user/bigdata/10_SQL/salaries' overwrite into table salary;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256bc6fb",
   "metadata": {},
   "source": [
    "Bestudeer nu opnieuw de [HQL language](https://cwiki.apache.org/confluence/display/Hive/LanguageManual) en stel de queries op die de volgende zaken opzoeken:\n",
    "* De 10 oudste werknemers\n",
    "* Het aantal werknemers dat gestart is in 1990\n",
    "* De voor en familienaam en gemiddelde salaris van de 10 werknemers met het hoogste gemiddelde salaris. (Tip: gebruik order by ipv sort by om globale orde te bepalen in reducer)\n",
    "* Is er een gender wage gap aanwezig in dit bedrijf? Bepaal hiervoor per geslacht het gemiddelde salaris aan de hand van een group by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ac11a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "select * from employee order by birthday asc limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c03a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "select count(*) from employee where work_day >= '1990-01-01' and work_day <= '1990-01-31';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98ac1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "select e.first_name, e.family_name, avg(s.salary) as avg_salary from\n",
    "    employee as e join salary as s on (e.employee_id == s.employee_id)\n",
    "        group by e.first_name, e.family_name order by avg_salary limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f524dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT e.gender, AVG(s.salary) AS avg_salary\n",
    "    FROM employee AS e\n",
    "          JOIN salary AS s\n",
    "            ON (e.employee_id == s.employee_id)\n",
    "GROUP BY e.gender;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb63acff",
   "metadata": {},
   "source": [
    "Buiten rechtstreeks te werken met de hive interface om rechtstreeks queries uit te voeren kunnen we dit ook met spark doen.\n",
    "Hieronder staan de bovenstaande commando's omgezet naar queries met spark om het binnen een spark applicatie uit te voeren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b75f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# warehouse_location points to the default location for managed databases and tables\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Spark_Hive_Les\").config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport().getOrCreate()\n",
    "\n",
    "spark.sql(\"\"\"create external table employee (employee_id INT, birthday DATE, first_name STRING, family_name STRING,\n",
    "          gender CHAR(1), work_day DATE) row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde' \n",
    "          stored as textfile USING hive;\"\"\")\n",
    "\n",
    "spark.sql(\"LOAD DATA INPATH '/user/bigdata/10_SQL/employees' overwrite into table employee;\")\n",
    "\n",
    "spark.sql(\"select * from employee order by birthday asc limit 10;\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
