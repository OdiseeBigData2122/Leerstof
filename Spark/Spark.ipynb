{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Spark\n",
    "\n",
    "Hoewel het MapReduce algoritme van Hadoop een aantal voordelen heeft. \n",
    "De meest beperkende eigenschap van het MapReduce algoritme is de snelheid.\n",
    "Omdat alles ingelezen wordt vanaf de harde schijf, tussenresultaten op de schijf opgeslagen worden en de finale resultaten ook wordt er tot wel 90% van de rekentijd gespendeerd in lees- of schrijfopdrachten.\n",
    "\n",
    "Spark is geintroduceerd om dit te versnellen door gebruik te maken van in-memory processing.\n",
    "Hierdoor is Spark tot 3 keer sneller op grote datasets en tot 100 keer op kleinere datasets.\n",
    "\n",
    "Het spark framework kan gebruik maken van een externe opslag-locatie voor bestanden bij te houden (zoals HDFS) en bestaat uit de volgende componenten:\n",
    "* SparkCore\n",
    "* Spark SQL\n",
    "* Spark Streaming\n",
    "* MLlib\n",
    "* SparkGraph\n",
    "\n",
    "Daarnaast zijn er ook verschillende Spark Api's voor verschillende programmeertalen zoals Python, Scala, Java, ...\n",
    "Hierdoor is het framework ook flexibeler dan het standaard MapReduce algoritme.\n",
    "Heel veel informatie over het spark framework vind je in de [documentatie](https://spark.apache.org/docs/latest/quick-start.html) en de programming guides (bovenaan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pydoop.hdfs as hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 09:44:56,032 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user/bigdata/06_Spark\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "localFS = hdfs.hdfs(host='')\n",
    "client = hdfs.hdfs(host='localhost', port=9000)\n",
    "\n",
    "if not client.exists('/user/bigdata/06_Spark'):\n",
    "    client.create_directory('/user/bigdata/06_Spark')\n",
    "client.set_working_directory('/user/bigdata/06_Spark')\n",
    "print(client.working_directory())\n",
    "\n",
    "# do some cleaning in case anything else than input is present on HDFS\n",
    "for f in client.list_directory(\".\"):\n",
    "    if not f[\"name\"].endswith(\"input.txt\"):\n",
    "        client.delete(f[\"name\"], True)\n",
    "        \n",
    "# upload input.txt\n",
    "localFS.copy(\"input.txt\", client, \"input.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Installatie\n",
    "\n",
    "Een python implementatie van Spark kan eenvoudig geinstalleerd worden door het volgende commando uit te voeren. \n",
    "Dit moet maar eenmalig gebeuren.\n",
    "Om te kijken of het reeds geinstalleerd is kan je kijken naar de versie van pyspark (indien geinstalleerd). \n",
    "Als de versie correct gereturned wordt, dan is het reeds geinstalleerd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/bigdata/hadoop/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.2.0\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.14\n",
      "Branch HEAD\n",
      "Compiled by user ubuntu on 2021-10-06T12:46:30Z\n",
      "Revision 5d45a415f3a29898d92380380cfd82bfc7f579ea\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark kan op drie manieren werken:\n",
    "* Boven op MapReduce (traag)\n",
    "* Boven op Yarn\n",
    "* Via zijn eigen resource manager\n",
    "\n",
    "In deze notebook gaan we gebruik maken van Spark gebruikmakende van yarn.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "# Spark Context -> die bevat informatie over waar de code moet uitgevoerd worden\n",
    "# Spark Conf -> configuratie van de applicatie / metadata / naam /aantal cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor de configuratie moeten we vooral twee zaken aangeven, namelijk:\n",
    "* Naam van de applicatie (is zichtbaar in de yarn)\n",
    "* Master url. De url dat het type cluster en hoe het te bereiken aangeeft. Wij gaan vooral werken met local om te communiceren met het lokale bestandssysteem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/bigdata/hadoop/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-03-17 08:41:52,794 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2022-03-17 08:42:09,586 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"wordcount\").setMaster(\"yarn\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# andere manier om de sparkcontext aan te maken\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# get de sparkcontext als hij reeds bestaat, anders maak je een nieuwe aan\n",
    "# er kan maar 1 sparkcontext tegelijkertijd actief zijn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Wordcount voorbeeld\n",
    "\n",
    "Om de api van pyspark te leren kennen kan je gaan naar de [documentatie](https://spark.apache.org/docs/latest/api/python/reference/index.html).\n",
    "Een eerder stap bij stap uitleg kan je [hier](https://spark.apache.org/docs/latest/api/python/getting_started/index.html) vinden.\n",
    "\n",
    "In onderstaande code gaan we stap voor stap het wordcount-voorbeeld uitwerken.\n",
    "\n",
    "Eerst moet er een pyspark context aangemaakt worden als volgt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# text file inlezen (dit is een dataframe)\n",
    "textFile = spark.read.text(\"06_Spark/input.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(value='Hello World,')\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# aantal rijen\n",
    "print(textFile.count())\n",
    "\n",
    "# geef de eerste rij\n",
    "print(textFile.first())\n",
    "\n",
    "# geef het aantal rijen waarin het woord world voorkomt\n",
    "print(textFile.filter(textFile.value.contains(\"world\")).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# wordcount example\n",
    "# splits elke lijn in woorden\n",
    "# sc.textFile -> geeft een rdd\n",
    "words = sc.textFile(\"06_Spark/input.txt\").flatMap(lambda line: line.split(\" \"))\n",
    "# zet elk woord om naar (woord, 1)\n",
    "wordcounts1 = words.map(lambda word: (word[0], 1) if len(word) > 0 else (\"\",1))\n",
    "# tel alle values op in het woord\n",
    "# wordt gedaan per woord afzonderlijk\n",
    "# a is de running count\n",
    "# (woord, 1)\n",
    "# (woord, 1)\n",
    "# a=0 , b=1 (value van het eerste woord) -> a+b = nieuwe a\n",
    "# a=1 , b=1 (value van het eerste woord) -> a+b = nieuwe a\n",
    "wordcounts2 = wordcounts1.reduceByKey(lambda a,b: a+b)\n",
    "wordcounts2.saveAsTextFile(\"06_Spark/output3.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wat gebeurt er in dit voorbeeld?**\n",
    "\n",
    "Sparkcontext om een connectie te maken met de distributed storage\n",
    "De input file wordt dan ingelezen met de textFile functie.\n",
    "Door middel van de flatMap functie wordt de tekst lijn per lijn ingelezen en gesplits in woorden. \n",
    "Dit resulteert in een RDD (Resilient Distributed Dataset.\n",
    "De .map() functie maakt een key-value pair aan voor elke keer dat het woord voorkomt.\n",
    "In een laatste fase is er een reduce stap per key die de som neemt van alle keren dat het woord voorkomt om de uiteindelijke wordcount te nemen.\n",
    "Of af te ronden wordt het resultaat opgeslagen.\n",
    "\n",
    "![spark wordcount in yarn](images/yarn_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession\n",
    "\n",
    "Nu gaan we stuk voor stuk de verschillende stappen bekijken om een pyspark applicatie te maken.\n",
    "De eerste stap is het aanmaken van een sessie (SparkSession) wat het beginpunt is voor spark applications.\n",
    "Er zijn twee manieren om een SparkSession aan te maken:\n",
    "* builder()\n",
    "* newSession()\n",
    "\n",
    "Bij het aanmaken van een session wordt er intern een SparkContext object aangemaakt. \n",
    "Dit object stelt de connectie naar een cluster voor.\n",
    "Er kan maar 1 context tegelijkertijd actief zijn.\n",
    "Als je wil connecteren met een tweede cluster moet je eerst stop() oproepen op de reeds actieve context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draai spark lokaal (niet op een cluster)\n",
    "# 1 omdat we 1 core willen toekennen\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"Spark_les\").getOrCreate()\n",
    "\n",
    "#spark.stop()\n",
    "\n",
    "# zonder argument in master draait op de cluster\n",
    "# hier kiezen we voor 2 cores\n",
    "spark2 = SparkSession.builder.config(\"spark.driver.cores\", 2).appName(\"Spark_les\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD\n",
    "\n",
    "Op basis van het SparkSession object is het dan mogelijk om RDD-objecten aan te maken.\n",
    "Een RDD is de basis dataobject binnen Spark dat in parallel op verschillende nodes binnen een cluster kan uitgevoerd worden.\n",
    "Alle dataobjecten binnen spark horen tot deze klassen en dus zijn er veel mogelijkheden om RDD's aan te maken.\n",
    "Hier haal ik er twee aan:\n",
    "* parallelize() om bestaande python objecten om te zetten naar een RDD\n",
    "* textFile() of andere read methoden om bestanden op de cluster uit te lezen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voorbeeld parallelize\n",
    "dataList = [(\"Student1\", 8), (\"Student2\", 16), (\"Student3\", 11)]\n",
    "rdd = spark.sparkContext.parallelize(dataList)\n",
    "\n",
    "# voorbeeld textFile\n",
    "rdd2 = spark.sparkContext.textFile(\"06_Spark/input.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Met bovenstaande methoden hebben we twee rdd's aangemaakt. \n",
    "Op deze objecten kunnen nu verscheidene operaties uitgevoerd worden.\n",
    "Een belangrijke eigenschap van dit type objecten is dat ze steeds in parallel uitgevoerd worden.\n",
    "\n",
    "De beschikbare operaties kunnen in twee groepen verdeeld worden:\n",
    "* transformaties\n",
    "* acties\n",
    "\n",
    "[Transformaties](https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-transformations/) zijn lazy-operations waarvoor de berekening uitgesteld wordt en geven een nieuw RDD terug.\n",
    "Een aantal voorbeelden van transformaties zijn:\n",
    "* flatMap()\n",
    "* map()\n",
    "* reduceByKey()\n",
    "* filter()\n",
    "* sortByKey()\n",
    "\n",
    "[Acties](https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-actions/) zijn operaties die een berekening starten (ook van de nodige transformaties) en geven een niet RDD-object terug. \n",
    "Een aantal voorbeelden hiervan zijn:\n",
    "* count()\n",
    "* collect()\n",
    "* first()\n",
    "* max()\n",
    "* reduce()\n",
    "\n",
    "Lees nu bovenstaande links en geef de functies die nodig zijn voor de volgende vragen op te lossen. Geef ook aan of het transformaties zijn of acties:\n",
    "* Het aantal keer dat elke waarde aanwezig is in de dataset (1 functie voor wordcount uit te voeren)\n",
    "* Uitfilteren van rijen\n",
    "* Groeperen van een aantal rijen op basis van een bepaalde waarde.\n",
    "* Toevoegen van een kolom aan elke key (bvb de lengte van een woord)\n",
    "* Hoe doe je head() uit pandas op RDD's?\n",
    "* Hoe doe je de apply() uit pandas op RDD's?\n",
    "\n",
    "Maak nu een spark applicatie dat van de eerste RDD (met de studenten) telt hoeveel studenten geslaagd zijn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda student: student[1] >= 10).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De applicatie voor het berekenen van een gemiddelde is iets complexer.\n",
    "Dit soort applicaties kan geschreven worden als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.666666666666666"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aantal elementen in rdd\n",
    "aantal = rdd.count()\n",
    "\n",
    "# elk element delen door dit aantal\n",
    "# iets doen voor elk element kan door de map() en reduce()\n",
    "rdd.map(lambda x: x[1]).reduce(lambda x,y: x+y) / aantal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schrijf nu een mapreduce applicatie in spark om de tweede RDD van de input te verwerken en het aantal woorden van elke lengte te bekomen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello World,', 'hello world,', 'hello world,', '', 'Dit is een voorbeeld file om het Wordcount voorbeeld te testen !']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World,', 'hello', 'world,', 'hello', 'world,', '', 'Dit', 'is', 'een', 'voorbeeld', 'file', 'om', 'het', 'Wordcount', 'voorbeeld', 'te', 'testen', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 1), (6, 1), (5, 1), (6, 1), (5, 1), (6, 1), (0, 1), (3, 1), (2, 1), (3, 1), (9, 1), (4, 1), (2, 1), (3, 1), (9, 1), (9, 1), (2, 1), (6, 1), (1, 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 4), (0, 1), (2, 3), (4, 1), (5, 3), (3, 3), (9, 3), (1, 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:=============================>                             (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(rdd2.collect())\n",
    "# map doet de lambda per element\n",
    "# flatMap doet een map maar zet dan alles in 1 lijst\n",
    "words = rdd2.flatMap(lambda line: line.split(\" \"))\n",
    "print(words.collect())\n",
    "wordcounts1 = words.map(lambda word: (len(word), 1))\n",
    "print(wordcounts1.collect())\n",
    "result = wordcounts1.reduceByKey(lambda a,b: a+b)\n",
    "print(result.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes\n",
    "\n",
    "Een belangrijke subklasse van RDD's zijn dataframes.\n",
    "Dit is een veel gebruikte manier om gestructureerde data voor te stellen.\n",
    "Dataframes in spark is sterk gerelateerd aan de dataframes gezien in pandas.\n",
    "Het belangrijskte verschil is dat ze verdeeld worden over de cluster en operaties op de dataframes in parallel uitgevoerd worden.\n",
    "Dataframes kunnen aangemaakt worden door gebruik te maken van de createDataFrame functie in context of ingelezen worden vanuit csv's of jsons. Ten slotte kunnen dataframes ook komen van externe bronnen zoals databases als resultaat van een sql-query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+------+---------+\n",
      "|firstname|lastname|       dob|gender|   budget|\n",
      "+---------+--------+----------+------+---------+\n",
      "|    Harry|  Potter|1980-07-31|     M|100000000|\n",
      "|   Ronald|   Wemel|1980-04-01|     M|       10|\n",
      "|Hermelijn| Griffel|1979-09-19|     F|     4000|\n",
      "+---------+--------+----------+------+---------+\n",
      "\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- budget: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+------+---------+\n",
      "|firstname|lastname|       dob|gender|   budget|\n",
      "+---------+--------+----------+------+---------+\n",
      "|    Harry|  Potter|1980-07-31|     M|100000000|\n",
      "|   Ronald|   Wemel|1980-04-01|     M|       10|\n",
      "|Hermelijn| Griffel|1979-09-19|     F|     4000|\n",
      "+---------+--------+----------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('Harry', 'Potter','1980-07-31','M',100000000),\n",
    "  ('Ronald','Wemel','1980-04-01','M',10),\n",
    "  ('Hermelijn','Griffel','1979-09-19','F',4000)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\", \"lastname\", \"dob\", \"gender\", \"budget\"]\n",
    "df = spark.createDataFrame(data = data, schema=columns)\n",
    "df.show()\n",
    "\n",
    "df.printSchema()\n",
    "# dit gaat het nog niet tonen\n",
    "# dit staat op de cluster, niet op de lokale machine\n",
    "df.describe()\n",
    "# dataframe lokaal hebben\n",
    "df.collect()\n",
    "# enkel printen\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark SQL\n",
    "\n",
    "Bovenstaande datastructuren (RDD's en Dataframes) zijn een onderdeel van het Pyspark sql module.\n",
    "De Spark API heeft een hele reeks methoden en functies om deze in te laden, uit te lezen en te manipuleren.\n",
    "Daarnaast maakt deze module het ook mogelijk om SQL-queries uit te voeren op dataframes.\n",
    "Om SQL-queries uit te voeren op dataframes moet er eerst een view gemaakt worden in het dataframe met de functie createOrReplaceTempView(\"view_name\")\n",
    "\n",
    "Daarna kan je gebruik maken van de .sql() functie om allerhande sql queries uit te voeren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+------+---------+\n",
      "|firstname|lastname|       dob|gender|   budget|\n",
      "+---------+--------+----------+------+---------+\n",
      "|    Harry|  Potter|1980-07-31|     M|100000000|\n",
      "|   Ronald|   Wemel|1980-04-01|     M|       10|\n",
      "+---------+--------+----------+------+---------+\n",
      "\n",
      "+---------+--------+----------+------+---------+\n",
      "|firstname|lastname|       dob|gender|   budget|\n",
      "+---------+--------+----------+------+---------+\n",
      "|    Harry|  Potter|1980-07-31|     M|100000000|\n",
      "|   Ronald|   Wemel|1980-04-01|     M|       10|\n",
      "+---------+--------+----------+------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|gender|count(1)|\n",
      "+------+--------+\n",
      "|     M|       2|\n",
      "|     F|       1|\n",
      "+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|     M|    2|\n",
      "|     F|    1|\n",
      "+------+-----+\n",
      "\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    Harry|  Potter|\n",
      "|   Ronald|   Wemel|\n",
      "|Hermelijn| Griffel|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# om sql queries uit te kunnnen voeren (maak een tabel test aan die het dataframe bijhoudt)\n",
    "df.createOrReplaceTempView(\"test\")\n",
    "df_male = spark.sql(\"select * from test where gender='M'\")\n",
    "df_male.show()\n",
    "\n",
    "# createOrReplaceTempView is hier niet voor nodig\n",
    "df_male2 = df.filter(df.gender == \"M\")\n",
    "df_male2.show()\n",
    "\n",
    "# groupby zaken tonen -> aantal van elk geslacht\n",
    "df_num_gender = spark.sql(\"select gender,count(*) from test group by gender\")\n",
    "df_num_gender.show()\n",
    "\n",
    "df_num_gender2 = df.groupby(\"gender\").count()\n",
    "df_num_gender2.show()\n",
    "\n",
    "# kolommen selecteren, of zaken gaan berekenen\n",
    "df.select(\"firstname\", \"lastname\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buiten de functionaliteit om SQL queries uit te voeren is ook het lezen en schrijven van allerhande dataformaten een belangrijk onderdeel van de pyspark sql module.\n",
    "Meer informatie hierover kun je [hier](https://spark.apache.org/docs/latest/sql-data-sources.html) vinden in de documentatie.\n",
    "In essentie ziet de code er uit als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"delimiter\", \";\").option(\"header\",True).option(\"multiline\", True).csv(\"{}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De opties die hierbij gekozen kunnen worden kun je vinden in de documentatie.\n",
    "\n",
    "Daarnaast zijn er ook functionaliteiten om data uit te lezen speciaal voor Machine Learning zoals libsvm en image-directories maar die worden later getoond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oefening\n",
    "\n",
    "Net zoals RDD kunnen er een aantal operaties uitgevoerd worden op deze dataframes.\n",
    "Een volledige lijst met alle operaties kan je [hier](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#dataframe-apis) vinden.\n",
    "Zoek de functies die gebruikt moeten worden om de volgende zaken uit te voeren:\n",
    "* Groepeer volgens een bepaalde sleutel\n",
    "* Krijg een lijst met alle kolomnamen\n",
    "* Filter rijen uit\n",
    "* Verwijder null-values in de dataset via rijen\n",
    "* Verwijder null-values door kolommen te verwijderen\n",
    "* Bereken een dataframe met statistieken van het dataframe\n",
    "* Krijg een dataframe met alle nan waarden\n",
    "* Hoe krijg je informatie zoals .info()\n",
    "* Hoe werkt het groeperen van informatie op basis van een key/kolom\n",
    "\n",
    "Probeer deze ook uit op bovenstaand aangemaakt dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby\n",
    "# .columns\n",
    "# filter()     / where() (alias)\n",
    "# dropna()\n",
    "# drop() voor de kolommen (dit gaat manueel moeten gedaan worden -> kijk eerst of er kolommen zijn met veel null-waarden)\n",
    "# describe()\n",
    "# isnan() -> eventueel een kolomnaam in \n",
    "   # -> is dit hetzelfde als isnull() \n",
    "       # null = geen data\n",
    "       # nan = data is geen nummer -> \"Harry\" in numerieke kolom\n",
    "# printSchema()/.schema\n",
    "# groupby() -> is een transformatie (gaat het df gaan verdelen in groepjes) wordt pas uitgevoerd als er een actie is\n",
    "    # een actie is -> count(), max(), ... (iets dat de groepen gaat samenvoegen tot 1 rij)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lees daarna volgende [link](https://sparkbyexamples.com/pyspark/pyspark-aggregate-functions/) om een idee te krijgen over hoe verschillende functies uit te voeren op deze dataframes.\n",
    "Werk nu de volgende oefening uit en maak hiervoor een spark applicatie:\n",
    "* Laad de kleine dataset over soorten irissen uit via sklearn en slap deze op als csv\n",
    "* Upload de csv naar de cluster\n",
    "* Schrijf de code om de csv uit te lezen en om te zetten naar een dataframe\n",
    "* Print het dataschema uit voor het dataframe en bepaal hoeveel kolommen er aanwezig zijn in deze dataset\n",
    "* Bereken de gemiddelde waarde van elke kolom\n",
    "* Bereken hoeveel waarden in elke kolom groter zijn dan het gemiddelde\n",
    "* Voeg een kolom toe dat bijhoudt hoeveel waarden op elke rij groter zijn dan het gemiddelde van die kolom\n",
    "* Selecteer de rijen waar het target/klasse/label gelijk is aan 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "df = load_iris(as_frame=True).frame\n",
    "df.to_csv(\"input.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not client.exists('/user/bigdata/06_Spark/demo'):\n",
    "    client.create_directory('/user/bigdata/06_Spark/demo')\n",
    "client.set_working_directory('/user/bigdata/06_Spark/demo')\n",
    "\n",
    "# do some cleaning in case anything else than input is present on HDFS\n",
    "for f in client.list_directory(\".\"):\n",
    "    if not f[\"name\"].endswith(\"input.csv\"):\n",
    "        client.delete(f[\"name\"], True)\n",
    "        \n",
    "# upload input.txt\n",
    "localFS.copy(\"input.csv\", client, \"input.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+-----------------+----------------+------+\n",
      "|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|target|\n",
      "+-----------------+----------------+-----------------+----------------+------+\n",
      "|              6.3|             3.3|              6.0|             2.5|     2|\n",
      "|              5.8|             2.7|              5.1|             1.9|     2|\n",
      "|              7.1|             3.0|              5.9|             2.1|     2|\n",
      "|              6.3|             2.9|              5.6|             1.8|     2|\n",
      "|              6.5|             3.0|              5.8|             2.2|     2|\n",
      "|              7.6|             3.0|              6.6|             2.1|     2|\n",
      "|              4.9|             2.5|              4.5|             1.7|     2|\n",
      "|              7.3|             2.9|              6.3|             1.8|     2|\n",
      "|              6.7|             2.5|              5.8|             1.8|     2|\n",
      "|              7.2|             3.6|              6.1|             2.5|     2|\n",
      "|              6.5|             3.2|              5.1|             2.0|     2|\n",
      "|              6.4|             2.7|              5.3|             1.9|     2|\n",
      "|              6.8|             3.0|              5.5|             2.1|     2|\n",
      "|              5.7|             2.5|              5.0|             2.0|     2|\n",
      "|              5.8|             2.8|              5.1|             2.4|     2|\n",
      "|              6.4|             3.2|              5.3|             2.3|     2|\n",
      "|              6.5|             3.0|              5.5|             1.8|     2|\n",
      "|              7.7|             3.8|              6.7|             2.2|     2|\n",
      "|              7.7|             2.6|              6.9|             2.3|     2|\n",
      "|              6.0|             2.2|              5.0|             1.5|     2|\n",
      "+-----------------+----------------+-----------------+----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(sepal length (cm)='6.7', sepal width (cm)='3.0', petal length (cm)='5.2', petal width (cm)='2.3', target='2', klasse='klasse 2'),\n",
       " Row(sepal length (cm)='6.3', sepal width (cm)='2.5', petal length (cm)='5.0', petal width (cm)='1.9', target='2', klasse='klasse 2'),\n",
       " Row(sepal length (cm)='6.5', sepal width (cm)='3.0', petal length (cm)='5.2', petal width (cm)='2.0', target='2', klasse='klasse 2'),\n",
       " Row(sepal length (cm)='6.2', sepal width (cm)='3.4', petal length (cm)='5.4', petal width (cm)='2.3', target='2', klasse='klasse 2'),\n",
       " Row(sepal length (cm)='5.9', sepal width (cm)='3.0', petal length (cm)='5.1', petal width (cm)='1.8', target='2', klasse='klasse 2')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%file spark_demo.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, sum, when, count\n",
    "\n",
    "# eerst een spark sessie aanmaken\n",
    "spark = SparkSession.builder.config(\"spark.driver.cores\", 2).appName(\"oefening_spark_les\").getOrCreate()\n",
    "\n",
    "# lees csv\n",
    "df = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv(\"/user/bigdata/06_Spark/demo/input.csv\")\n",
    "# df.printSchema()\n",
    "df = df.drop(\"_c0\") # inplace gaat niet, alles maakt altijd een nieuw dataframe aan\n",
    "\n",
    "# bereken het gemiddel van elke kolom\n",
    "#df.select(avg(\"sepal length (cm)\")).show()\n",
    "#df.select(avg(\"sepal length (cm)\"), avg(\"sepal width (cm)\")).show()\n",
    "\n",
    "cols = [\"sepal length (cm)\", \"sepal width (cm)\"]\n",
    "#df.select([avg(c) for c in cols]).show()\n",
    "df_avg = df.select([avg(c).alias(c) for c in df.columns]).collect()[0]\n",
    "\n",
    "# aantal waarden groter dan het gemiddelde in die kolom\n",
    "# 1. op zoek gaan naar welke waarden groter dan het gemiddelde\n",
    "# 2. tel al deze op\n",
    "\n",
    "# df_avg[0] is het gemiddelde van de eerste kolom\n",
    "df_larger_than_avg = df.select([(col(c) > df_avg[index]).alias(c) for index, c in enumerate(df.columns)])\n",
    "#df_larger_than_avg.select([sum(col(c).cast(\"int\")) for c in df_larger_than_avg.columns]).show()\n",
    "#df_larger_than_avg.select([count(when(col(c), 1)) for c in df_larger_than_avg.columns]).show()\n",
    "\n",
    "# extra kolom met het aantal op die rij groter dan het gemiddelde\n",
    "# kolom toevoegen doe je met withColumn(\"naam\", hoe ze te bereken)\n",
    "tmp = df_larger_than_avg.select([col(c).cast(\"int\").alias(c) for c in df.columns])\n",
    "cols = tmp.columns\n",
    "df2 = tmp.withColumn(\"aantal\", col(cols[0]) + col(cols[1]) + col(cols[2]) + col(cols[3]) + col(cols[4]))\n",
    "#df2.show(5)\n",
    "\n",
    "# target label is 2\n",
    "df.filter(df.target == 2).show()\n",
    "df.withColumn(\"klasse\", when(col(\"target\") == 0, \"klasse 0\")\n",
    "              .when(col(\"target\") == 1, \"klasse 1\")\n",
    "             .otherwise(\"klasse 2\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/bigdata/hadoop/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-03-17 09:49:40,994 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2022-03-17 09:49:48,758 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2022-03-17 09:51:05,827 WARN csv.CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , sepal length (cm), sepal width (cm), petal length (cm), petal width (cm), target\n",
      " Schema: _c0, sepal length (cm), sepal width (cm), petal length (cm), petal width (cm), target\n",
      "Expected: _c0 but found: \n",
      "CSV file: hdfs://localhost:9000/user/bigdata/06_Spark/demo/input.csv\n",
      "+---+-----------------+----------------+-----------------+----------------+------+\n",
      "|_c0|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|target|\n",
      "+---+-----------------+----------------+-----------------+----------------+------+\n",
      "|  0|              5.1|             3.5|              1.4|             0.2|     0|\n",
      "|  1|              4.9|             3.0|              1.4|             0.2|     0|\n",
      "|  2|              4.7|             3.2|              1.3|             0.2|     0|\n",
      "|  3|              4.6|             3.1|              1.5|             0.2|     0|\n",
      "|  4|              5.0|             3.6|              1.4|             0.2|     0|\n",
      "|  5|              5.4|             3.9|              1.7|             0.4|     0|\n",
      "|  6|              4.6|             3.4|              1.4|             0.3|     0|\n",
      "|  7|              5.0|             3.4|              1.5|             0.2|     0|\n",
      "|  8|              4.4|             2.9|              1.4|             0.2|     0|\n",
      "|  9|              4.9|             3.1|              1.5|             0.1|     0|\n",
      "| 10|              5.4|             3.7|              1.5|             0.2|     0|\n",
      "| 11|              4.8|             3.4|              1.6|             0.2|     0|\n",
      "| 12|              4.8|             3.0|              1.4|             0.1|     0|\n",
      "| 13|              4.3|             3.0|              1.1|             0.1|     0|\n",
      "| 14|              5.8|             4.0|              1.2|             0.2|     0|\n",
      "| 15|              5.7|             4.4|              1.5|             0.4|     0|\n",
      "| 16|              5.4|             3.9|              1.3|             0.4|     0|\n",
      "| 17|              5.1|             3.5|              1.4|             0.3|     0|\n",
      "| 18|              5.7|             3.8|              1.7|             0.3|     0|\n",
      "| 19|              5.1|             3.8|              1.5|             0.3|     0|\n",
      "+---+-----------------+----------------+-----------------+----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!python spark_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shared variabelen**\n",
    "\n",
    "Variabelen met read-write acces zijn zeer inefficient om te gebruiken in een cluster met sterke parallelisatie.\n",
    "Spark bied echter twee varianten aan die wel efficient geimplementeerd kunnen worden, namelijk\n",
    "* Broadcasted variabelen\n",
    "* Accumulators\n",
    "\n",
    "Broadcasted variabelen zijn read-only variabelen, die aangemaakt worden door de driver en eenmalig verspreid worden over de nodes in plaats van voor elke job.\n",
    "Dit wordt vooral gebruikt om grote data die veelvuldig gebruikt wordt te cachen op de nodes.\n",
    "Bij het gebruik van broadcast variabelen is het belangrijk om te onthouden dat je de originele variabele niet meer mag gebruiken na het aanmaken van de broadcasted variabele omdat ze anders toch nog elke job doorgestuurd wordt.\n",
    "De belangrijkste functies om te werken met broadcasted variabelen zijn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dit is een broadcasted string'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcasted = spark.sparkContext.broadcast(\"dit is een broadcasted string\")\n",
    "broadcasted.value\n",
    "broadcasted.unpersist() # tijdelijke delete -> kan opnieuw doorgetuurd worden wanneer nodig\n",
    "broadcasted.destroy() # permanente delete, kan niet meer gebruikt worden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accumulators**\n",
    "\n",
    "Het andere type dat aangeboden wordt zijn accumulators.\n",
    "Deze laten enkel toe dat noden iets toevoegen aan een gedeelde variabele.\n",
    "Enkel de driver kan deze variabele uitlezen.\n",
    "Dit kan bijvoorbeeld gebruikt worden om tellers of sommen bij te houden.\n",
    "Deze accumulators kunnen een naam hebben (named accumulators zijn zichtbaar in de wep api).\n",
    "De ingebouwde accumulator van Spark ondersteunt enkel numerieke accumulators.\n",
    "Het is echter mogelijk om eigen accumulators toe te voegen door over te erven van de AccumulatorParam klasse en deze twee functies te implementeren:\n",
    "* zero: De begin waarde van de accumulator\n",
    "* addInPlace: Om twee waarden samen te voegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "16\n",
      "[8, 16, 11]\n"
     ]
    }
   ],
   "source": [
    "#dataList = [(\"Student1\", 8), (\"Student2\", 16), (\"Student3\", 11)]\n",
    "\n",
    "# numeriek\n",
    "accumVar = spark.sparkContext.accumulator(0)\n",
    "rdd.foreach(lambda x: accumVar.add(x[1]))\n",
    "print(accumVar.value)\n",
    "\n",
    "# find max score\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "class MaxAccumulator(AccumulatorParam):\n",
    "    def zero(self, value):\n",
    "         return value\n",
    "    \n",
    "    def addInPlace(self, var, new_value):\n",
    "        if var < new_value:\n",
    "            return new_value\n",
    "        return var\n",
    "    \n",
    "accumVar = spark.sparkContext.accumulator(0, MaxAccumulator())\n",
    "rdd.foreach(lambda x: accumVar.add(x[1]))\n",
    "print(accumVar.value)\n",
    "\n",
    "# add to list\n",
    "class ListAccumulator(AccumulatorParam):\n",
    "    def zero(self, value):\n",
    "        return []\n",
    "    \n",
    "    def addInPlace(self, var, new_value):\n",
    "        var.extend(new_value)\n",
    "        return var\n",
    "    \n",
    "# note that to store a list, lists must be passed to initial value and add\n",
    "accumVar = spark.sparkContext.accumulator([], ListAccumulator())\n",
    "rdd.foreach(lambda x: accumVar.add([x[1]]))\n",
    "print(accumVar.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "Door de hoge populariteit van pandas in python is er een alternatief uitgewerkt binnen de laatste versie van Spark (eind 2021) dat de pandas api integreert.\n",
    "Hierdoor kan je code schrijven die identiek is aan te werken met pandas.\n",
    "Lees [dit artikel](https://towardsdatascience.com/run-pandas-as-fast-as-spark-f5eefe780c45) om meer informatie te krijgen over de verschillen tussen de dataframes API en de pandas-on-spark API.\n",
    "De documentatie voor deze api vind je [hier](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html).\n",
    "Een belangrijk onderdeel van deze documentatie omvat de best practices:\n",
    "* Check execution plan (dmv de explain() functie)\n",
    "* Use checkpoints voor fout-tolerantie en efficientie van de planner.\n",
    " * df.spark.local_checkpoint()\n",
    "* Vermeid data shuffling (sorting) omdat hierbij data tussen nodes moet gestuurd worden wat niet efficient is.\n",
    "* Vermeid berekeningen op 1 partitie (geen parallellisatie)\n",
    "* Vermeid kolomnamen startend of eindigend op \"_\"\n",
    " * Deze worden gebruikt door interne functies van pandas/spark\n",
    "* Kolomnamen moeten uniek zijn\n",
    "* Specificeer de index kolom bij omzetten dataframe en pandas-on-spark API\n",
    "* Gebruik zoveel mogelijk van de pandas-on-spark API direct ipv standaard python functies om conflicten te vermijden\n",
    " * df.sum() werkt maar sum(df) niet\n",
    "* Vermeid operaties op meerdere dataframes want deze gebruiken een join intern en is hierdoor traag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
